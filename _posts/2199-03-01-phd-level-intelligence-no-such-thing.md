---
title: 'Why comparing GPT-5 to a PhD is high comedy'
subtitle: 'On tech bros, science experts, and the chasm between them'
date: 2199-03-01
permalink: /posts/2199-03-01-phd-level-intelligence-no-such-thing/
tags:
  - large-language-models
---

How would you define "expertise", and what does it _really_ mean to be an expert on a topic? 

I was sitting at a traffic light in my car the first time I heard Sam Altman announce GPT-5 as having “PhD-level intelligence,” and I involuntarily laughed out loud to myself. There are so many problems with this that it's hard to know where to start---but I'll start with the obvious: intelligence and knowledge are not the same thing. ChatGPT possesses a great deal of _knowledge_ on a wide range of subjects, and I've personally found it to be a useful aggregator of this knowledge.[^1] But ChatGPT remains, at its core, a fancy plagiarism machine subject to the occasional acid flashback. I had already begun working on this essay when <a href="https://bsky.app/profile/shengokai.bsky.social/post/3lvxfci6jms2w" target="_blank">a recent, articulate Bluesky post</a> by Jonathan Flowers really brought it home for me. But let's dive a little deeper on this anyway, because I think there's a lot to unpack here.

<h2>Intelligence as a fallacy</h2>

It'll be brief, but we have to start with this: _intelligence_ isn't a monolithic concept, and it's widely debated by cognitive scientists whether you can measure it at all [citation]. The very concept has roots in eugenics [citation], and even modern intelligence quotient (IQ) tests are known to have significant cultural, racial, and class biases [citation]. In reality, one's intelligence is sort of an amorphous amalgamation of various types of cognitive abilities, and the level of these abilities vary from person to person. While we could sit here and debate what kind of cognitive abilities exist and which of these apply to GPT-5, even if we came to some kind of science-supported consensus, we'd still be missing the bigger picture. So let's quickly abandon the notion of "PhD-level intelligence" as the absurdity that it is and focus on another similar, somewhat more testable claim that has been made: that GPT-5 has "PhD-level _expertise_." 

<h2>What is a PhD good for, anyway?</h2>

I write this humbly as someone who does not have a PhD. I’m currently in the second year of my doctoral program, yet I can say with certainty that earning a PhD has almost nothing to do with raw intelligence and everything to do with perseverance, humility, and a relentless (sometimes irrational) curiosity about the world. We like to romanticize science as a noble pursuit of truth for its own sake, and while reality is often messier, a truth does emerge: most researchers are not in it for the money [citation]. Crucially, seeking knowledge out of curiosity---for its own sake---is something self-avowed techno-capitalists like Sam Altman will scarcely understand, let alone value.[^2]

Breadth of knowledge does not equate to depth of knowledge. While many PhDs know a little about a lot of topics thanks to their innate curiosity, they also tend to recognize when they are stepping outside their domain. The best will freely admit, “I don’t know.” That humility—often born from years of wrestling with problems so difficult that no one has solved them before—is almost entirely absent from the world of tech hype. 

<h2>Mind the gap</h2>

I've written previously about "the curse of knowledge" [citation] and there's a similar concept that comes into play here. 

There is a documented chasm that exists between tech bros and science experts, and that chasm is called the <a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect" target="_blank">Dunning–Kruger</a> effect. It's not remotely a stretch to apply this here, as you'll see. The Dunning-Kruger effect basically states that individuals with limited expertise (i.e., knowledge, experience) in a particular subject area generally tend to overestimate the degree of their own expertise, while people with a high degree of expertise tend to provide a much more accurate self-assessment of theirs... and often even _underestimate_ it. Put another way: the more you know about a topic, the more you realize just how much there is to know about it. 

It might be obvious to you why this is an apt analogy, but to put a fine point on it: the Dunning-Kruger theory suggests that the only person who can accurately assess the level of expertise a person has in a particular subject is an expert in that subject. I posit that we can extend this to machines as well. How exactly then can we trust college-dropout Sam Altman to assess whether GPT-5 has "PhD-level expertise" in any subject other than maybe, possibly, but probably not even computer science? If we believe Dunning-Kruger (even if we recognize that things are a bit more complex than the classical interpretation of the phenomena [citation]), then we should see his and the rhetoric of others in the LLM space more as marketing hype than as any kind of objective, realistic evaluation of their products. 

<h2>TBD</h2>

Let's assume for a moment that Dunning-Kruger is complete BS and that you don't buy into it at all. Fine. There's still a glaring problem with conflating a large language model with a domain expert. The pursuit of a PhD is, at its core, about the pursuit of answers to questions that haven't yet been answered. This is not the same as mining information from some database of knowledge, or even synthesizing that knowledge---on the contrary, it is about doing that, and then extrapolating using experimentation or some other meticulous investigatory methods. And if there's one thing we know about machine learning algorithms, it's that they don't extrapolate well [citation]. But that's beside the point, as even with all the "reasoning" trickery going on with GPT-5, we can be certain it's not performing experiments or applying rigorous statistical tests to data in the background [citation]. By all accounts [what is it doing?]. 

<h2>Storming the castle gates</h2>

Another argument that's been used to hype GPT is that it <a href="https://openai.com/index/ai-as-the-greatest-source-of-empowerment-for-all/" target="_blank">"democratizes" knowledge or expertise</a>. While OpenAI started as a non-profit, the money virus quickly infested the brains of its progenitors and, nowadays, your guess is as good as mine as to how a proprietary SaaS product is supposed to democratize anything.[^3] But I digress. The real issue here is that knowledge is already democratized in the form of amenities such as public libraries, Wikipedia, and open access journals. Yes, we have work to do on that last one, but regardless: what GPT offers is access to a search engine that is often wrong, while fooling its users (and even its creators) into thinking it is infallible through overconfidence and convincingly articulate language. In that respect, it might be more akin to a conservative infotainment platform like PragerU than it is to an encyclopedia.[^4]

At any rate, I feel like this argument very likely has something to do with the real and perceived gatekeeping that is, admittedly, one of the bugs of academia. I'll readily admit that this is a legitimate concern. But I'd sooner see us give our money to an organization like Wikipedia that actually cares about democratizing knowledge than one that is hell-bent on inventing a tool to replace humans in the workforce. Aside from the obvious problem that capitalism sort of needs consumers to exist, consumers sort of need money to consume, and their jobs provide them with said money... we're also finding that this <a href="https://www.sciencedirect.com/science/article/pii/S0007681324000582" target="_blank">just doesn't work</a>.

Some might accuse me of gatekeeping, and yes, academia can have its share of egos who believe the letters after their name elevate them to a higher intellectual plane. That’s just as misguided as thinking a language model has joined our ranks because it can mimic our output. A PhD isn’t a badge of omniscience; it’s a testament to the perseverance, humility, and curiosity required to solve one problem no one else has solved before. Those qualities don’t exist in predictive text. And no amount of slick marketing will change that.

Contrast that with ChatGPT, which is wrong often, and wrong confidently. <a href="https://www.theguardian.com/australia-news/2025/aug/08/openai-chatgpt-5-struggled-with-spelling-and-geography" target="_blank">The Guardian documented GPT-5 struggling with basic spelling and geography</a>—hardly the stuff of an expert mind. And yet the model will rarely, if ever, say “I don’t know.” Instead, it produces answers that sound plausible but are invented wholesale. This is because ChatGPT is trained on <em>past discoveries</em>; a PhD, by definition, is about making <em>new</em> discoveries. Being a doctoral-level expert isn’t about recalling a corpus of human knowledge—it’s about pushing beyond it.

Random thoughts:
* The "chasm" I refer to in the subtitle could refer perfectly to Dunning-Kruger
* Breadth of knowledge does not equate to depth of knowledge. While their innate curiosity often means they'll know a little about a great many topics, most PhDs I've met will readily admit when they're straying outside their narrow area of expertise.
* Perseverence is about getting to the answer to a problem that is difficult - so difficult that no one has answered it before. This is what getting a PhD is about.
* Humility is sorely lacking in the tech world
* Seeking knowledge out of curiosity is something self-avowed capitalists will really never understand or appreciate
* Mention that ChatGPT is wrong, a lot. Experts can be wrong, but they don't make things up. Experts say "I don't know". 
* Mention the Guardian's findings (https://www.theguardian.com/australia-news/2025/aug/08/openai-chatgpt-5-struggled-with-spelling-and-geography)
* Mention the Bluesky past that discussed this:
  * https://bsky.app/profile/shengokai.bsky.social/post/3lvxfci6jms2w
* Mention the fact that ChatGPT is trained on _past discoveries_, and being a PhD-level expert is about making new discoveries.
* Cynics might say this is about gatekeeping, which certainly can be a thing in academia. I've met many PhDs who are full of themselves and think this honorary means they exist on a different intellectual plane than others. This is wrong. 

So, how would you define expertise? [discussion about how it's not memorizing a bunch of things]

Make no mistake: GPT-5 is not "artificial general intelligence" - not remotely. But by continuing to refer to these large language models with terms like _Generative AI_, Altman and his ilk want you to believe that they're much more than they are: sophisticated plagiarism machines that interpolate from a vast body of knowledge that itself arose through human perseverence, humility, and a relentless (sometimes irrational) curiosity about the world. Nothing that I see on the tech horizon has the ability to replace that.

Not yet, anyway.

[^1]: As long as you, uh, fact check it, because it is very often completely and hopelessly wrong [citation].
[^2]: For some great insight into another techno-capitalist with absolutely no demonstrable curiosity about the world, check out this <a href="https://www.richardcarrier.info/archives/33301" target="_blank">insightful essay</a> by Richard Carrier.
[^3]: I won't get into the recent open source releases because <a href="https://venturebeat.com/ai/the-initial-reactions-to-openais-landmark-open-source-gpt-oss-models-are-highly-varied-and-mixed/" target="_blank">the jury still seems to be out</a> on them. 
[^4]: If not for its inherent bias toward the left of the polical spectrum. Darn those meddling leftists and their long-standing concern for knowledge and truth!